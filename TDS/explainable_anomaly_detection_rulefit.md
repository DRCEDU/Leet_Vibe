# Explainable Anomaly Detection with RuleFit: An Intuitive Guide

*Published on Towards Data Science, July 4, 2025*  
*By Shuai Guo*

## Overview

This article introduces the RuleFit algorithm as a solution for making anomaly detection explainable. RuleFit generates interpretable IF-THEN rules that characterize anomalies, providing stakeholders with clear explanations for why a sample is considered anomalous.

---

## 1. How Does RuleFit Work?

RuleFit consists of two main steps:

### 1.1 The "Rule" in RuleFit
- Rules are generated by training an ensemble of decision trees (e.g., random forest, gradient boosting).
- Each path from the root to a leaf in a tree forms an IF-THEN rule (e.g., IF x1 < 10 AND x2 > 5 THEN 1 ELSE 0).
- The process creates a diverse pool of candidate rules.

### 1.2 The "Fit" in RuleFit
- Each rule is treated as a binary feature (1 if the rule applies, 0 otherwise).
- RuleFit performs sparse linear regression (Lasso) using both the original features and the binary rule features to predict the target outcome.
- Lasso sets coefficients of unimportant features to zero, effectively selecting the most important rules.
- The magnitude of the coefficients indicates rule importance.

### 1.3 Recap
- Step 1: Extract rules from decision trees (the "Rule" part).
- Step 2: Use Lasso regression to select and rank important rules (the "Fit" part).
- Surviving rules with non-zero coefficients are the most important for explaining the outcome.

---

## 2. Anomaly Explanation with RuleFit

### 2.1 Application Pattern
- Convert the unsupervised anomaly detection problem into a supervised one by labeling detected anomalies as 1 and normal points as 0.
- Use RuleFit to generate and select rules that explain what makes a sample anomalous.
- Alternatively, use the anomaly score as the target for regression, to explain the severity of anomalies.
- Binary label classification is more common for generating actionable business rules.

### 2.2 Case Study (Iris Dataset)
- Anomaly detection is performed using the Local Outlier Factor (LOF) algorithm.
- Detected anomalies are labeled, and RuleFit is applied to explain these anomalies.
- RuleFit is implemented using the `imodels` Python package:

```python
from imodels import RuleFitClassifier
rf = RuleFitClassifier(max_rules=30, lin_standardise=True, include_linear=True, random_state=42)
rf.fit(X_test, y_pred, feature_names=X_test.columns)
```

- The rules are extracted and ranked by importance:

```python
rules = rf._get_rules()
rules = rules[rules.coef != 0]
rules = rules[~rules.type.str.contains('linear')]
rules['abs_coef'] = rules['coef'].abs()
rules = rules.sort_values('importance', ascending=False)
```

- Example result: A rule such as "If petal length > 5.45 cm and petal width > 2 cm, the odds of being classified as anomalous increase 85-fold."
- The support column shows the fraction of samples where the rule applies.
- The importance score combines the coefficient magnitude and the rule's discriminative power.

---

## 3. Conclusion

- RuleFit provides a two-step approach for explainable anomaly detection: extracting rules from decision trees and ranking them via sparse linear regression.
- It enables practitioners to answer "Why is this anomaly?" with concrete, interpretable IF-THEN rules.

---

## References

1. Jerome H. Friedman, Bogdan E. Popescu, Predictive learning via rule ensembles, arXiv, 2008.
2. Fisher, R. A., Iris [Data set]. UCI Machine Learning Repository, 1936.

---

For more details and the full article, see:  
[Explainable Anomaly Detection with RuleFit: An Intuitive Guide](https://towardsdatascience.com/explainable-anomaly-detection-with-rulefit-an-intuitive-guide/) 