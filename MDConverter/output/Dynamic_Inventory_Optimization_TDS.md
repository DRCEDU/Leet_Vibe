---
title: "Dynamic Inventory Optimization with Censored Demand | Towards Data Science"
og_title: "Dynamic Inventory Optimization with Censored Demand | Towards Data Science"
description: "A sequential decision framework with Bayesian learning"
url: "https://towardsdatascience.com/dynamic-inventory-optimization-with-censored-demand-2/"
published: "2025-07-14T22:11:24+00:00"
author: "Mert Ersoz"
---

# Dynamic Inventory Optimization with Censored Demand | Towards Data Science

## Article Information

**Author:** Mert Ersoz
**Published:** 2025-07-14T22:11:24+00:00
**Original URL:** https://towardsdatascience.com/dynamic-inventory-optimization-with-censored-demand-2/

**Description:** A sequential decision framework with Bayesian learning

---

[Data Science](https://towardsdatascience.com/category/data-science/)


# Dynamic Inventory Optimization with Censored Demand

A sequential decision framework with Bayesian learning

[Mert Ersoz](https://towardsdatascience.com/author/numersoz/)

Jul 14, 2025

20 min read

Share

![Image](images/82d61016-c99c-4b6d-9c12-26e8da1fdb20.jpg)

Image Generated by Author via DALL-E

We often make decisions under uncertainty. Not just once, but in a sequence over time. We rely on our past experiences and expectations of the future to make the most informed and optimal choices possible.

Think of a business that offers several products. These products are procured at a cost and sold for a profit. However, unsold inventory may incur a restocking fee, may carry salvage value, or in some cases, must be scrapped entirely.

Businesses, therefore, faces a crucial question: how much to stock? This decision must often be made before demand is fully known; that is, under censored demand. If the business overstocks, it observes the full demand, since all customer requests are fulfilled. But if it understocks, it only sees that demand exceeded supply and the actual demand remains unknown, making it a censored observation.

![Image](images/image-42-683x1024.png)

Image Generated by Author via DALL-E

This type of problem is often referred to as a **Newsvendor Model**. In fields such as operations research and applied mathematics, the optimal stocking decision has been studied by framing it as a classic newspaper stocking problem; hence the name.

In this article, we explore a **Sequential Decision-Making** framework for the stocking problem under uncertainty and develop a dynamic optimization algorithm using Bayesian learning.

Our approach closely follows the framework laid out by [Warren B. Powell, Reinforcement Learning and Stochastic Optimization (2019)](https://castle.princeton.edu/wp-content/uploads/2019/10/Powell-Reinforcement-Learning-and-Stochastic-Optimization.pdf) and implements the paper from [Negoescu, Powell, and Frazier (2011), Optimal Learning Policies for the Newsvendor Problem with Censored Demand and Unobservable Lost Sales](https://people.orie.cornell.edu/pfrazier/pub/learning_newsvendor.pdf), published in Operations Research.


## Problem Setup

Following a similar setup to that of Negoescu et al., we frame the problem as optimizing the inventory level for a single item over a sequence of time steps. The cost and selling price are considered fixed. Unsold inventory is discarded with no salvage value, while each unit sold generates revenue. Demand is unknown, and when the available stock is less than actual demand, the demand observation is considered censored.

Demand W in each period is drawn from an exponential distribution with an unknown rate parameter, for simulation purposes.

xWλcp∈R+∼Exponential(λ):Order quantity (decision variable):Random demand with unknown rate parameter λ:Demand rate (unknown, to be estimated):Unit cost to procure or produce the item:Unit selling price (assume p>c for profitability)

The parameter λ in the exponential distribution represents the **rate of demand**; that is, how quickly demand events occur. The **average demand** is given by E[W]=1λ.

![Image](images/image-95.png)

Image by Author

We can observe from the **Probability Density Function** (PDF) of the Exponential distribution that higher values of demand W become less likely. Thus, the Exponential distribution serves as an appropriate choice for demand modeling.


## Sequential Decision Formulation

We formulate the inventory control problem as sequential decision process under uncertainty. The goal is to maximize total expected profit over a finite time horizon N, while learning unknown demand rate by applying Bayesian Learning principals.

We define a model with an initial state and a probabilistic model that represents its belief about future states over time. At each time step, the model makes a decision based on a policy that maps its current belief to an action. The goal is to find the **optimal policy** that maximizes a predefined reward function.

After taking an action, the model observes the resulting state and updates its belief, accordingly, continuing this cycle of decision, observation, and belief update.

![Image](images/diagram.drawio-2.png)


### 1) State Variable

We model demand in each period as a random variable drawn from an Exponential distribution with an unknown rate parameter λ. Since λ is not directly observable, we encode our uncertainty about its value using a Gamma prior:

λ∼Gamma(a0,b0)

The parameters a0 and b0 define the shape and rate of our initial belief about the demand rate. These two parameters serve as our **state variables**. At each time step, they summarize all past information and are updated as new demand observations become available.

As we collect more data, the posterior distribution over lambda evolves from a wide and uncertain shape to a narrower and more confident one, gradually concentrating around the true demand rate.

This process is captured naturally by the Gamma distribution, which flexibly adjusts its shape based on the amount of information we’ve seen. Early on, the distribution is diffuse, signaling high uncertainty. As observations accumulate, the belief becomes sharper, allowing for more reliable and responsive decision-making. Probability Density Function (PDF) of Gamma distribution can be seen below:

![Image](images/image-96.png)

Image by Author

We will later define a **transition function** that updates the state, that is, how (an,bn) evolves to (an+1,bn+1), based on newly observed data. This allows the model to continuously refine its belief about demand and make more informed inventory decisions over time.

Note that expected value of the Gamma distribution is defined as:

E[λ]=ab


### 2) Decision Variable

The **decision variable** at time n is the stocking level:

xn∈R+

This is the number of units to order before demand Wn+1 is realized. The decision depends only on the current belief (an,bn).


### 3) Exogenous Information

After selecting xn, demand Wn+1 is revealed:

Wn+1∼Exp(λ)

Since λ is unknown, demand is random. Observations are:


- **Uncensored** if Wn+1<xn (we observe the actual demand)

- **Censored** if Wn+1≥xn (we only know demand exceeding supply levels)

This censoring limits the information available for belief updating. Even though the full demand isn’t observed, the censored observation still carries valuable information and should not be ignored in our modeling approach.


### 4) Transition Function

The transition function defines how the model’s belief, represented by the state variables, is updated over time. It maps the prior state to the expected future state, and in our case, this update is governed by Bayesian learning.


#### Bayesian Uncertainty Modelling

Bayes’ theorem combines prior belief with observed data to form a posterior distribution. This updated distribution reflects both prior knowledge and the newly observed information.

pn+1(λ∣wn+1)=p(wn+1∣λ)⋅pn(λ)p(wn+1)

Where:

p(wn+1∣λ): Likelihood of new observation at time n+1

pn(λ): Prior at time n

p(wn+1): Marginal likelihood (normalizing constant) at time n+1

pn+1(λ∣wn+1): Posterior after observing wn+1

We set up our problem such that in each period, demand W is drawn from an Exponential distribution. Prior belief over λ will be modelled using a Gamma distribution.

pn+1(λ∣wn+1)=λe−λwn+1Likelihood⋅bannΓ(an)λan–1e−bnλPrior (Gamma)∫∞0λe−λwn+1⋅bannΓ(an)λan–1e−bnλdλMarginal (evidence)

The Gamma and Exponential distributions form a well-known **conjugate prior** in Bayesian statistics. When using a Gamma prior and an Exponential likelihood, the resulting posterior is also a Gamma distribution. This property of the prior and posterior belonging to the same distributional family is what defines a conjugate prior. Posterior also belongs to the Gamma family; a property that simplifies Bayesian updating significantly.

For reference, closed-form conjugate updates like this can be found in standard conjugate prior tables, such as the one on [Wikipedia](https://en.wikipedia.org/wiki/Conjugate_prior). Using this reference, we can formulate the posterior as:

Let:

λ∣w1,…,wn∼Gamma(a0+n, b0+∑i=1nwi)

λ∼Gamma(a0,b0): Prior

w∼Exp(λ): Likelihood

For *n* independent observations w1,…,wn, the Gamma prior and Exponential likelihood result in a Gamma posterior:

After observing a single (uncensored) demand w, the posterior simplifies to below by leveraging conjugate priors:

λ∣w∼Gamma(a0+1, b0+w)


- The shape parameter increases by **1** because one new data point has been observed.

- The rate parameter increases by **w** because the Exponential likelihood includes the term e−λw, which combines with the prior’s exponential term and adds to the total exponent.


#### The Update Function

The posterior parameters (state variables) are updated based on the nature of the observation:


- **Uncensored** (Wn+1<xn):

an+1=an+1,bn+1=bn+Wn+1


- **Censored** (Wn+1≥xn):

an+1=an,bn+1=bn+xn

These updates reflect how each observation, full or partial, informs the posterior belief over λ.

We can define the transition function in Python as below:

```
```python
from typing import Tuple

def transition_a_b(
 a_n: float,
 b_n: float,
 x_n: float,
 W_n1: float
) -> Tuple[float, float]:
 """
 Updates the posterior parameters (a, b) after observing demand.

 Args:
 a_n (float): Current shape parameter of Gamma prior.
 b_n (float): Current rate parameter of Gamma prior.
 x_n (float): Order quantity at time n.
 W_n1 (float): Observed demand at time n+1 (may be censored).

 Returns:
 Tuple[float, float]: Updated (a_{n+1}, b_{n+1}) values.
 """
 if W_n1 < x_n:
 # Uncensored: full demand observed

 a_n1 = a_n + 1
 b_n1 = b_n + W_n1
 else:
 # Censored: only know that W >= x

 a_n1 = a_n
 b_n1 = b_n + x_n

 return a_n1, b_n1
```
```


### 5) Objective Function

The model seeks a policy π, mapping beliefs to stocking decisions in order to maximize total expected profit.


- Profit from ordering xn units and facing demand Wn+1:

F(xn,Wn+1)=p⋅min(xn,Wn+1)–c⋅xn


- The cumulative objective is:

maxπE[∑n=0N−1F(xn,Wn+1)]


- π maps (an,bn) to xn

- p is the selling price per unit sold

- c is the unit cost of ordering

- Unsold units are discarded with no salvage value

Note that this objective function maximizes only the expected immediate reward across the entire time horizon. In the next section, we introduce an expanded version that incorporates the value of future learning. This encourages the model to explore, accounting for the information that censored demand can reveal over time.

We can define the profit function in Python as below:

```
```python
def profit_function(x: float, W: float, p: float, c: float) -> float:
 """
 Profit function defined as:

 F(x, W) = p * min(x, W) - c * x

 This represents the reward received when fulfilling demand W with inventory x,
 earning price p per unit sold and incurring cost c per unit ordered.

 Args:
 x (float): Inventory level / decision variable.
 W (float): Realized demand.
 p (float, optional): Unit selling price.
 c (float, optional): Unit cost.

 Returns:
 float: The profit (reward) for this period.
 """
 return p * min(x, W) - c * x
```
```


## Policy Functions

We will define several policy functions as defined by Negoescu et al, which will update the value of xn+1 (stocking level) based on our current belief of the state (an,bn).


### 1) Point Estimate Policy

Under this policy, model estimates the unknown demand rate λ using the current posterior and chooses and order quantity xn+1 to maximize the immediate expected profit.

At time n, current posterior about λ Gamma(an,bn) is:

λ^n=anbn

We treat this estimate as the “true” value of λ and assume demand W∼Exp(λ^n).


#### Expected Value

The profit for order quantity x and realized demand W is:

F(x,W)=p⋅min(x,W)–c⋅x

We seek to maximize the expected profit.

maxx≥0EW[pmin(x,W)–cx]

Expected value of a random variable is:

E[X]=∫∞−∞x⋅f(x)dx

Thus, the objective function can be written as:

maxx≥0[p(∫x0wfW(w)dw+x∫∞xfW(w)dw)–cx]

Where:


- fW(x): Probability density function (PDF) of demand evaluated at x

PDF of Exponential(λ) is:

fW(w)=λ^ne−λ^nw

This can be solved as:

E[F(x,W)]=p⋅1–e−λ^nxλ^n–cx


#### First Order Optimality Condition

We set the derivative of the expected profit function to zero, and solve for x to find the stocking value which maximize the expected profit:

ddxE[F(x,W)]=pe−λ^nx–c=0

e−λ^nx∗=cp⇒x∗=1λ^nlog(pc)

Substitute λ^n=anbn:

xn=bnanlog(pc)

Python implementation:

```
```python
import math

def point_estimate_policy(
 a_n: float,
 b_n: float,
 p: float,
 c: float
) -> float:
 """
 Point Estimate Policy, chooses x_n based on posterior mean at time n.

 Args:
 a_n (float): Gamma shape parameter at time n.
 b_n (float): Gamma rate parameter at time n.
 p (float): Selling price per unit.
 c (float): Unit cost.

 Returns:
 float: Stocking level x_n
 """
 lambda_hat = a_n / b_n
 return (1 / lambda_hat) * math.log(p / c)
```
```


### 2) Distribution Policy

The Distribution Policy optimizes the expected immediate profit by integrating over the entire current belief distribution of the demand rate λ. Unlike the Point Estimate Policy, it does not collapse the posterior to a single value.

At time n, the belief about λ is:

λ∼Gamma(an,bn)

Demand is modelled as:

W∼Exp(λ)

This policy chooses order quantity xn by maximizing the expected immediate profit, averaged over both the uncertainty in demand and the uncertainty in λ

xn=argmaxx≥0 Eλ∼Gamma(an,bn)[EW∼Exp(λ)[p⋅min(x,W)–cx]]

This is the expected immediate profit, averaged over both the uncertainty in demand **and** the uncertainty in λ.


#### Expected Value

From the previous policy, we know that:

EW[min(x,W)]=1–e−λ^nxλ^n

Thus:

Eλ[EW∣λ[min(x,W)]]=Eλ[1–e−λxλ]

If we denote the Gamma density as:

f(λ)=baΓ(a)λa–1e−bλ

Then expectation becomes:

Eλ[1–e−λxλ]=∫∞01–e−λxλf(λ)dλ=baΓ(a)∫∞0(1–e−λx)λa–2e−bλdλ

Without going over the full proof, expectation becomes:

E[Profit]=p⋅Eλ[1–e−λxλ]–cx=p⋅ba–1(1–(bb+x)a–1)–cx


#### First Order Optimality Condition

Again, we set the derivative of the expected profit function to zero, and solve for x to find the stocking value which maximize the expected profit:

ddxE[Profit]=ddx[p⋅ba–1(1–(bb+x)a–1)–cx]=0

Without going over the proof, closed form expresion based on Negoescu et al’s paper is:

xn=bn((pc)1/an–1)

Python implementation:

```
```python
def distribution_policy(
 a_n: float,
 b_n: float,
 p: float,
 c: float
) -> float:
 """
 Distribution Policy, chooses x_n by integrating over full posterior at time n.

 Args:
 a_n (float): Gamma shape parameter at time n.
 b_n (float): Gamma rate parameter at time n.
 p (float): Selling price per unit.
 c (float): Unit cost.

 Returns:
 float: Stocking level x_n
 """
 return b_n * ((p / c) ** (1 / a_n) - 1)
```
```


### Knowledge Gradient (KG) Policy

The Knowledge Gradient (KG) policy is a Bayesian learning policy that balances *exploitation* (maximizing immediate profit) and *exploration* (ordering to gain information about demand for future decisions).

Instead of just maximizing today’s profit, KG chooses the order quantity that maximizes:

**Profit now + Value of information gained for the future**

xn=argmaxx E[p⋅min(x,Wn+1)–cx+V(an+1,bn+1)∣an,bn,x]

Where:


- Wn+1∼Exp(λ) (with λ∼Gamma(an,bn))

- V(an+1,bn+1) is the value of expected future profits under updated beliefs after observing Wn+1

We do not know an+1,bn+1 at time n because we haven’t yet observed demand. So, we compute their expected value under the possible observation outcomes (censored vs uncensored).

The KG policy then evaluates each candidate stocking quantity x by:


- Simulating its effect on posterior beliefs

- Computing the immediate profit

- Computing the value of future learning based on belief updates


#### Objective Function

We define the total value of choosing x at time n as:

FKG(x)=E[p⋅min(x,W)–cx]Immediate profit+(N–n)⋅Eposterior[maxx′Eλ∼posterior[p⋅min(x′,W)–cx′]]Value of learning


- The first term is just expected immediate profit.

- The second term accounts for how this choice improves future profit by sharpening our belief about λ.

- Horizon Factor (N−n): We will make (N−n) more decisions in the future. So the value of better decisions due to learning today gets multiplied by this factor.

- Posterior Averaging Eposterior[⋅]: This means we are averaging over all the possible posterior beliefs we might end up with after observing the outcome of demand; because demand is random and possibly censored, we won’t get perfect information, but we will update our belief.

The paper uses previously discussed Distribution Policy as proxy for estimating the Future Value function. Thus:

x∗(a,b)=V(a,b)=bpa–1(1–(bb+x∗)a–1)–cx∗=b((pc)1/a–1)


#### Expected Value

Expected value of V is expressed as below per Negoescu et al. As the proof of this equation is quite complex, we’ll not be going over the details.

E[V]=E[E[bn+1(pan+1–1(1–(cp)1–1an+1)–c((cp)−1an+1–1))∣∣λ]∣∣an,bn,xn]=E[∫xn0(bn+y)(pan(1–(cp)1–1an+1)–c((cp)−1an+1–1))λe−λydy+∫∞xn(bn+xn)(pan–1(1–(cp)1–1an)–c((cp)−1an–1))λe−λydy].

As we already know the expected value of the immediate profit function as described under previous policies, we can express the additive expected value of KG policy as summation. As this equation is quite long, we’ll not be going over the details, but it can be found in the paper.


#### First Order Optimality Condition

In this policy as well, we set the derivative of the expected profit function to zero, and solve for x to find the stocking value which maximize the expected profit. Closed form solution of the equation based on the paper is:

xn=bn⎡⎣⎢⎢⎛⎝⎜r1+(N–n)⋅(1+anran–1–(an+1)ran)⎞⎠⎟−1/an–1⎤⎦⎥⎥

Where:


- r=cp: Cost to price ratio

Python implementation:

```
```python
def knowledge_gradient_policy(
 a_n: float,
 b_n: float,
 p: float,
 c: float,
 n: int,
 N: int
) -> float:
 """
 Knowledge Gradient Policy, one-step lookahead policy for exponential demand
 with Gamma(a_n, b_n) posterior.

 Args:
 a_n (float): Gamma shape parameter at time n.
 b_n (float): Gamma rate parameter at time n.
 p (float): Selling price per unit.
 c (float): Unit cost per unit.
 n (int): Current period index (0-based).
 N (int): Total number of periods in the horizon.

 Returns:
 float: Stocking level x_n
 """
 a = max(a_n, 1.001) # Avoid division by zero for small shape values

 r = c / p

 future_factor = (N - (n + 1)) / N
 adjustment = 1.0 - future_factor * (1.0 / a)
 adjusted_r = min(max(r * adjustment, 1e-4), 0.99)

 return b_n * ((1 / adjusted_r) ** (1 / a) - 1)
```
```


### Monte Carlo Policy Evaluation

To evaluate a policy π in a stochastic environment, we simulate its performance over multiple sample demand paths.

Let:


- M be the number of independent simulations (demand paths), each denoted ωm for m=1,2,…,M

- N be the time horizon

- pn(ωm) be the simulated price at time n on path m

- xn(ωm) be the decision taken at time n under policy π on path n


#### Cumulative Reward on a Single Path

For each sample path ωm, compute the total reward:

F^π(ωm)=∑n=0N−1[p⋅min(xn(ωm),Wn+1(ωm))–c⋅xn(ωm)]

This represents the realized value of the policy π along that specific trajectory.

Python implementation:

```
```python
import numpy as np

def simulate_policy(
 N: int,
 a_0: float,
 b_0: float,
 lambda_true: float,
 policy_name: str,
 p: float,
 c: float,
 seed: int = 42
) -> float:
 """
 Simulates the sequential inventory decision-making process using a specified policy.

 Args:
 N (int): Number of time periods.
 a_0 (float): Initial shape parameter of Gamma prior.
 b_0 (float): Initial rate parameter of Gamma prior.
 lambda_true (float): True exponential demand rate.
 policy_name (str): One of {'point_estimate', 'distribution', 'knowledge_gradient'}.
 p (float): Selling price per unit.
 c (float): Procurement cost per unit.
 seed (int): Random seed for reproducibility.

 Returns:
 float: Total cumulative reward over N periods.
 """
 np.random.seed(seed)
 a_n, b_n = a_0, b_0
 rewards = []

 for n in range(N):
 # Choose order quantity based on specified policy

 if policy_name == "point_estimate":
 x_n = point_estimate_policy(a_n=a_n, b_n=b_n, p=p, c=c)
 elif policy_name == "distribution":
 x_n = distribution_policy(a_n=a_n, b_n=b_n, p=p, c=c)
 elif policy_name == "knowledge_gradient":
 x_n = knowledge_gradient_policy(a_n=a_n, b_n=b_n, p=p, c=c, n=n, N=N)
 else:
 raise ValueError(f"Unknown policy: {policy_name}")

 # Sample demand

 W_n1 = np.random.exponential(1 / lambda_true)

 # Compute profit and update belief

 reward = profit_function(x_n, W_n1, p, c)
 rewards.append(reward)

 a_n, b_n = transition_a_b(a_n, b_n, x_n, W_n1)

 return sum(rewards)
```
```


#### Estimate Expected Value by Averaging

The expected reward of policy π is approximated using the sample average across all M simulations:

F¯π=1N∑m=1NF^π(ωm)

This F¯π is an unbiased estimator of the true expected reward under policy π.

Python implementation:

```
```python
import numpy as np

def policy_monte_carlo(
 N_sim: int,
 N: int,
 a_0: float,
 b_0: float,
 lambda_true: float,
 policy_name: str,
 p: float = 10.0,
 c: float = 4.0,
 base_seed: int = 42
) -> float:
 """
 Runs multiple Monte Carlo simulations to evaluate the average cumulative reward
 for a given inventory policy under exponential demand.

 Args:
 N_sim (int): Number of Monte Carlo simulations to run.
 N (int): Number of time steps in each simulation.
 a_0 (float): Initial Gamma shape parameter.
 b_0 (float): Initial Gamma rate parameter.
 lambda_true (float): True rate of exponential demand.
 policy_name (str): Name of the policy to use: {"point_estimate", "distribution", "knowledge_gradient"}.
 p (float): Selling price per unit.
 c (float): Procurement cost per unit.
 base_seed (int): Seed offset for reproducibility across simulations.

 Returns:
 float: Average cumulative reward across all simulations.
 """
 total_rewards = []

 for i in range(N_sim):
 reward = simulate_policy(
 N=N,
 a_0=a_0,
 b_0=b_0,
 lambda_true=lambda_true,
 policy_name=policy_name,
 p=p,
 c=c,
 seed=base_seed + i
 )
 total_rewards.append(reward)

 return np.mean(total_rewards)
```
```

```
```python

# Parameters

N_sim = 10000 # Number of simulations

N = 100 # Number of time periods

a_0 = 10.0 # Initial shape parameter of Gamma prior

b_0 = 5.0 # Initial rate parameter of Gamma prior

lambda_true = 0.25 # True rate of exponential demand

p = 26.0 # Selling price per unit

c = 20.0 # Unit cost

base_seed = 1234 # Base seed for reproducibility

results = {
 policy: policy_monte_carlo(
 N_sim=N_sim,
 N=N,
 a_0=a_0,
 b_0=b_0,
 lambda_true=lambda_true,
 policy_name=policy,
 p=p,
 c=c,
 base_seed=base_seed
 )
 for policy in ["point_estimate", "distribution", "knowledge_gradient"]
}

print(results)
```
```


### Results

![Image](images/image-97.png)
![Image](images/image-99.png)

The left plot shows how average cumulative profit evolves over time, while the right plot shows the average reward per time step. From this simulation, we observe that the Knowledge Gradient (KG) policy significantly outperforms the other two, as it optimizes not only immediate rewards but also the future value of cumulative rewards. Both the Point Estimate and Distribution policies perform similarly.

![Image](images/image-102-1024x606.png)

We can observe from above plots that the Bayesian Learning algorithm gradually converges to the true mean demand W.

These findings highlight the importance of incorporating the value of information in sequential decision making under uncertainty. While simpler heuristics like the Point Estimate and Distribution policies focus solely on immediate gains, the Knowledge Gradient policy leverages future learning potential, yielding superior long-term performance.

---

Written By

Mert Ersoz

[See all from Mert Ersoz](https://towardsdatascience.com/author/numersoz/)

[Bayesian Learning](https://towardsdatascience.com/tag/bayesian-learning/), [Deep Dives](https://towardsdatascience.com/tag/deep-dives/), [Demand Forecasting](https://towardsdatascience.com/tag/demand-forecasting/), [Reinforcement Learning](https://towardsdatascience.com/tag/reinforcement-learning/), [Supply Chain Analytics](https://towardsdatascience.com/tag/supply-chain-analytics/)

Share This Article


- [Share on Facebook](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-inventory-optimization-with-censored-demand-2%2F&title=Dynamic%20Inventory%20Optimization%20with%20Censored%20Demand)

- [Share on LinkedIn](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-inventory-optimization-with-censored-demand-2%2F&title=Dynamic%20Inventory%20Optimization%20with%20Censored%20Demand)

- [Share on X](https://x.com/share?url=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-inventory-optimization-with-censored-demand-2%2F&text=Dynamic%20Inventory%20Optimization%20with%20Censored%20Demand)

Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.

[Write for TDS](https://towardsdatascience.com/questions-96667b06af5/)